

== Hackathons/Workshops
=== December 2018 Workshop/Hackathon in Northern Virginia
==== Background
In December 2018, the MetOeaan DWG conducted a two day API workshop/hackathon event in Northern Virginia.  The goal of the workshop was to introduce the weather on the web data API work as a REST-based query pattern making use of the OpenAPI specification and discuss how the API tries to simplify the need to understand the OGC WCS standard or the need to know about Meteorological or Oceanographic terms.  That the API was simply a quick means to an end goal, the product.   Discussion topics included the URL request/response patterns, the query patterns, and user experiences making use of the demo APO.   The full agenda for the workshop/hackathon can be found [[here]].  Participants of the workshop included OGC staff, a number of Met Center Agencies, academia, and corporations. A complete listing of the participants can be found [[here]].
==== What was acheived
1.	Created a github collaborative workspace environment for MetOcean API:   https://github.com/OGCMetOceanDWG/MetOceanAPI-Workshop 
2.	Demonstration of initial WOTW capability, focused on gridded data
3.	OpenAPI demo endpoint independent testing (Canadian Centre for Meteorological and Environmental Prediction)
 a.	Successfully conducted first pass testing against a generic WFS 3.0 client library (OWSLib - https://github.com/geopython/OWSLib)
 b.	Provided recommendations to align closer to WFS 3.0 patterns in github: https://github.com/OGCMetOceanDWG/MetOceanAPI-Workshop/blob/master/api-review-comments.md
4.	Initial cut at WOTW API conformance classes (See Conformance Class file)
 1.	Core (0):  Use simple feature specification (point, line string, polygon).  How do we interpret this in terms of parameters?  How do we encode this?  Start value, end value, interval, Begin, end, enumeration.  This would be same for time and points.  CRS is fixed on 4326.  Parameter list would be a list of WMO vocabulary (Links to 2 Grib tables)
 2.	Spatial (1):  required for start, stop, interval and enumeration.  Not required for start and stop.  Specify default for interpolation, vertical, time.  For cross section and corridors use line string terminology (harder case though).  
 3.	Weather parameters (2): Use WFS as the basis for query parameters.  Create filter.  The API should be self describing and therefore advertise what parameters are available.  There will likely be restrictions to output encodes for Grib vs. json.
 4.	Temporal (3);  Use MetOcean specification, Grib2, netCDF
 5.	Measurements (4): 
 6.	Interpolation (5): Must specify interpolation method per axis.  Must use well known method.  Specify default as nearest neighbor.  Work on other interpolations methods with vendors 
 7.	Return(6): We inform.  Json, covjson, grib, netcdf, xml 
 8.	Vendor Extras (7):  Outside of scope.
5.  Initial cut at supported MetOcean API geometry query types (See: query pattern mappings file)
 1.	Point, point series
 2.	Polygon
 3.	Profile
 4.	grid
 5.	Corridor
 6.	Trajectory

== Implementations

Various related implementations already exist.

=== SOFP WFS3 Weather Server

Finnish Meteorological Institute (FMI), Vaisala and Spatineo have developed a Proof-of-Concept OGC Core - Features (WFS3) server. The main goals of the project are to experiment feasibility of WFS3 in providing weather observations and forecasts and a new _O&M Simple Feature Encodings_-data model (OMFS).

Used encoding of OMFS data model is based on GeoJSON. It is designed to ease the use of environmental data in web-based applications without still keeping a required (minimal) level of metadata and semantics. The data model enables data sharing and processing with a variety of existing technologies having simple feature handling capabilities. The data model is one alternative to consider in _Maintenance and Implementation Work Programme 2016-2020, Action 2017.2 on alternative encodings for INSPIRE data_.

The API has been tested in several ways. One implementation is available in FMI Open Data portal with selected data sets. Feedback is also gathered in _INSPIRE Helsinki 2019_ event where project partners have a challenge _Commuting 2.0_ to boost environmentally-friendly commuting. Both FMI and Vaisala have open implementations available during the challenge.

The server is implemented with NodeJS and TypeScript. The server architecture is modular: the core takes care of API and data encoding while different data-store integrations extract and process requested information from the underlying data sources such as files, databases or other services.

[%header,cols=2*]
|===
|Content
|Link

|Server core source code
|https://github.com/vaisala-oss/sofp-core

|FMI data integration source code
|https://github.com/fmidev/smartmet-sofp-backend

|OMSF profile repository
|https://github.com/opengeospatial/omsf-profile
|===

=== Meteorological Service of Canada GeoMet OGC API

In support of the https://canada.ca/climate-services[Canadian Centre for Climate Services] (CCCS), the Meteorological Service of Canada (MSC), as part of their https://www.canada.ca/en/environment-climate-change/services/weather-general-tools-resources/weather-tools-specialized-data/geospatial-web-services.html[GeoMet] platform, deployed an initial offering of weather, climate and water data via the OGC API - Features standard.  Made available in 2018, initial datasets included historical hydrometric and climate data.

Data was encoded as GeoJSON and was also provided via HTML representation.  The API was used as part of the CCCS https://climate-change.canada.ca/climate-data/[Climate data extraction tool], allowing users to visualize and access/download archive data with spatial, aspatial and temporal criteria for just in time data extraction of data relevant to their use case.  Future phases of the API will include real time weather observations and hydrometric data.  The API is also used for climate data extraction via the recently launched collaborative portal at https://climatedata.ca[ClimateData.ca].

The server is implemented with in Python with https://pygeoapi.io[pygeoapi] and provides a robust plugin architecture for extensibility.  Supported backends include Elasticsearch, PostgreSQL/PostGIS and GeoPackage.

[%header,cols=2*]
|===
|Content
|Link

|OGC API - Feature endpoint
|https://geo.weather.gc.ca/geomet/features

|Source code
|https://github.com/geopython/pygeoapi

|Website
|https://pygeoapi.io
|===

=== Met Office candidate specification demonstrator

The proposed candidate specification takes a very different (and hopefully complimentary) approach to working with geospatial data to the existing OGC API's.  Rather than focusing and providing rich query interfaces to expose the information available from the underlying datasets, the specification focuses on a small set of standard query patterns which treats all data as geo-temporally referenced data values with a consistent approach to the metadata which describes what is meant by those values.  The concept is there would be a (small) range of API query patterns in an attempt to keep the number of query parameters available to minimum and where possible limit the value options for those parameters to either a enumerated list or range of values which are supplied by the metadata responses from the API.  

The Met Office has developed a basic prototype to test the proposed candidate specification, this prototype had two main aims:

 - Demonstrate accessing data from a variety of datasources which include both Feature and Coverage types.

 - Test the idea that users would be able to retrieve data with no information other than that provided by the OpenAPI definition of the candidate API specification and the metadata available from the API. 

The prototype was limited to just the basic Point and Polygon queries with just one output format.  No attempt was made to make the prototype scalable as the focus was on providing access to a variety of datasources rather than query optimisation.

The server was developed using python due to the wide range of software libraries available for accessing scientific datasources.  To simplify the deployment process the application was built as a docker container, resources provided for the container are limited (1 CPU core and <6Gb or ram) in order to help identify possible resource bottlenecks.

One of the main aims of the candidate specification is to abstract the user from the underlying data structures, whilst providing a simple self describing query interface.  In the demo application the following underlying datasets were used as datasources to test the concept:

- METAR Observation data (feature)
- OpenStreetMap Highways data (feature)
- Shuttle Radar Topography Mission (SRTM) Digital Elevation Model (DEM) data (coverage)
- UK Met Office UKV Model (UKV) surface data (coverage)  
- UK Met Office Global Model surface data (coverage)
- NOAA Global Forecast System (GFS) 0.25 degree data (coverage)
- NWS National Digital Forecast Database (NDFD) (coverage) 

A variety of storage options were accessed with an aim to demonstrate the abstraction of the data structure and storage from the end user.  The UKV and NDFD data are stored as files on the server file system, the UK Global model files and Digital Elevation data are stored and accessed from Amazon S3 buckets; the GFS, Metar and OpenStreetMap data are accessed using various third party API's.

As well as providing a simple to explain interface for the user the limited range of accepted input values for the query parameters helped to simplify the server implementation. The most complicated input parameters were the COORDS and time, but the use of Well Known Text (WKT) and ISO8601 standards meant it was possible to use existing software libraries for parsing and validation of those values. As the only valid inputs for the remaining parameters are values that are supplied in the metadata returned by the API, the validation code could rely a simple comparison to the expected value list.

Another core principle of the candidate specification is to create an API which can be fully described by the OpenAPI 3.0 standard. This meant that it was possible to use off the shelf tools to produce developer friendly documentation, there are are also tools which will generate skeleton code in multiple languages; Those tools generate code which can query the API and parse the results returned, this proved very useful in testing that the implementation performed as described.



[%header,cols=2*]
|===
|Content
|Link

|Candidate spec API - Feature endpoint
|http://labs.metoffice.gov.uk/wotw/
